{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb24b958",
   "metadata": {},
   "source": [
    "### Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02638df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "import logging\n",
    "import os, csv\n",
    "import datetime, time\n",
    "from langdetect import detect\n",
    "\n",
    "#nltk\n",
    "import nltk, gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# gensim\n",
    "from gensim.utils import tokenize\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import  models\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b670c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f83f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, re, spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5afa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "# IMPORTANT: RISKY MOVE WHEN IMPORTING THIS FILE IN ANOTHER NOTEBOOK\n",
    "# BEWARE OF OVERRIDING GLOBAL VARIABLES\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "custom_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183604b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expressions\n",
    "\n",
    "html_cleaner = re.compile('<.*?>')\n",
    "emoji_cleaner = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "non_ascii = re.compile(\"[^\\x00-\\x7f]\")\n",
    "email_cleaner = re.compile('\\S*@\\S*\\s?')\n",
    "punctuation_cleaner = re.compile('[^\\w\\s]')\n",
    "only_alphabetic = re.compile('[^a-zA-Z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9a847",
   "metadata": {},
   "source": [
    "### Noise removal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea5bcec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(review):\n",
    "    return re.sub(r'[0-9]', \"\", review)\n",
    "def remove_nan(reviews_arr, is_text: False):\n",
    "    if is_text:\n",
    "        return re.sub('nan', '',reviews_arr)\n",
    "    return [re.sub('nan', '', review) for review in reviews_arr]\n",
    "def remove_html(reviews_arr, is_text: False):\n",
    "    if is_text:\n",
    "        return re.sub(html_cleaner, '', reviews_arr)\n",
    "    return [re.sub(html_cleaner, '', review) for review in reviews_arr]\n",
    "def remove_emoji(reviews_arr, is_text=False):\n",
    "    if is_text:\n",
    "        return re.sub(emoji_cleaner, '',reviews_arr)\n",
    "    return [re.sub(emoji_cleaner, '', review) for review in reviews_arr]\n",
    "def remove_email(reviews_arr, is_text: False):\n",
    "    if is_text:\n",
    "        return re.sub(email_cleaner, '',reviews_arr)\n",
    "    return [re.sub(email_cleaner, '',review) for review in reviews_arr]\n",
    "\n",
    "def remove_non_alphabetic(words, return_arr=False):\n",
    "    return [re.sub(only_alphabetic, '', word) for word in words]\n",
    "def remove_punctuation(words):\n",
    "    return [re.sub(punctuation_cleaner, \"\", word) for word in words]\n",
    "def remove_non_ascii(text):\n",
    "    return re.sub(non_ascii,'', text)\n",
    "def remove_new_lines(reviews_arr):\n",
    "    return [re.sub('\\s+', ' ', review) for review in reviews_arr]\n",
    "def remove_single_quotes(reviews_arr):\n",
    "    return [re.sub(\"\\'\", \"\", review) for review in reviews_arr]\n",
    "def remove_double_quotes(reviews_arr):\n",
    "    return [re.sub(\"\\\"\", \"\", review) for review in reviews_arr]\n",
    "\n",
    "\n",
    "# @param: list of sentences \n",
    "def clean_data(review_arr):\n",
    "    rev_sentences = remove_nan(review_arr)\n",
    "    rev_sentences = remove_double_quotes(remove_single_quotes(rev_sentences))\n",
    "    rev_sentences = remove_emoji(rev_sentences)\n",
    "    rev_sentences = remove_html(rev_sentences)\n",
    "    rev_sentences = remove_email(rev_sentences)\n",
    "    rev_sentences = remove_new_lines(rev_sentences)\n",
    "    return rev_sentences\n",
    "\n",
    "def clean_text(review):\n",
    "    # remove numbers, nan\n",
    "    formatted = remove_num(review)\n",
    "    formatted = remove_nan(formatted, True)\n",
    "    # remove html, email, emoji\n",
    "    formatted = remove_html(formatted, True)\n",
    "    formatted = remove_email(formatted, True)\n",
    "    formatted = remove_emoji(formatted, True)\n",
    "    return formatted\n",
    "\n",
    "# list of words - a single sentence in a review\n",
    "def clean_sent_words(sent_words, min_word_len = 0):\n",
    "    formatted = remove_punctuation(sent_words)\n",
    "    formatted = remove_non_alphabetic(formatted)\n",
    "    return list(filter(lambda x: len(x) > min_word_len, formatted)) # do not return empty word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb08207",
   "metadata": {},
   "source": [
    "### Sentence tokenize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34cecf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits strings into a list of words\n",
    "def nltk_tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "# returns list of list: each sentence broken down into words\n",
    "# simple_preprocess is gensim function\n",
    "def gensim_sent_to_words(reviews_arr):\n",
    "    for review_sent in reviews_arr:\n",
    "        yield(simple_preprocess(str(review_sent), deacc=True)) # deacc True removes punctuations\n",
    "def gensim_tokenize(text):\n",
    "    return list(tokenize(text)) # removes punctuations as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdd1fc",
   "metadata": {},
   "source": [
    "### Stopwords removal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc35b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_remove_stopwords(words, stopwords = custom_stop_words):\n",
    "    word_arr = [word for word in words if word not in stopwords]\n",
    "    return word_arr\n",
    "def gensim_remove_stopwords(texts, stopwords = custom_stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]    \n",
    "\n",
    "# not added - meditation, 'empower your ride your way', mindfulness, simple habit sleep meditation', grab\n",
    "appnames = [\"app\", \"application\", \"acorns\", \"acorn\", \"acorm\", \"atom\", \"betterment\", \"bitmart\", \"blockfi\", \"e-trade\", \"etrade\", \"e trade\", \"fidelity\", \"fidelity spire\", \"spire\", \"gemini\", \"ibkr\",\n",
    "               \"m1\", \"m1 finance\", \"marketsim\", \"kucoin\", \"personal capital\", \"power e trade\", \"public com\", \"rally rd\", \"robinhood\", \"rb\", \"schwab\", \"stash\",\n",
    "                \"stockpile\", \"stockwits\", \"td\", \"ameritrade\", \"thinkorswim\", \"think or swim\", \"titan\", \"vanguard\", \"wealthfront\", \"wealthsimple\", \"wealth simple\",\n",
    "                \"webull\", \"aura\", \"betterhelp\", \"dare\", \"calm\", \"happify\", \"happy not perfect mind\", \"headspace\", \"ibreathe\", \"insight timer\", \"lumosity\", \"meditation studio\",\n",
    "                \"mooodfit\", \"moodmission\", \"mood mission\", \"nocd\", \"regain\", \"rootd\", \"sanvello\", \"shine\", \"slumber\", \"talkspace\", \"smiling mind\", \"tenpercenthappier\", \"ten percent happier\",\n",
    "                \"wakingup\", \"whatsup\", \"woebot\", \"wysa\", \"youper\", \"zen\", \"99 private\", \"ado boletos\", \"beat\", \"blabla\", \"blablacar\", \"bolt\", \"curb\", \"didi\",\n",
    "                \"cabify\", \"flywheel\", \"gett\", \"hopskipdrive\", \"lyft\", \"kakao\", \"moovit\", \"blue bird\", \"ola\", \"rapido\", \"revel\", \"uber\", \"veyo\", \"via\", \"waze\", \"wingz\", \"ztrip\",\n",
    "               \"tj\"]\n",
    "spells = [\"u\", \"ish\"]\n",
    "aux_verbs = [\"be\", \"do\", \"have\", \"will\", \"shall\", \"should\", \"would\", \"could\", \n",
    "             \"may\", \"might\", \"must\", \"can\", \"ought\"]\n",
    "non_useful = [\"let\", \"maybe\", \"finally\", \"yeah\", \"oh\", \"man\", \"else\", \"elsewhere\", \n",
    "              \"definitely\", \"actually\", \"else where\", \"lot\", \"still\", \"even\",\n",
    "              \"dollars\", \"pounds\", \"euros\", \"dollar\", \"euro\", \"pound\",\n",
    "              \"really\", \"hey\", \"lol\", \"lot\", \"xoxo\", \"already\"]\n",
    "\n",
    "def extend_stopwords():\n",
    "    print(\"----------adding new keywords to custom_stop_words-----------\")\n",
    "    custom_stop_words.extend(appnames)\n",
    "    custom_stop_words.extend(spells)\n",
    "    custom_stop_words.extend(aux_verbs)\n",
    "    custom_stop_words.extend(non_useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427e029",
   "metadata": {},
   "source": [
    "### Token lemmatize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b83ba665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_lemmatize(word_arr, return_arr = True):\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in word_arr]\n",
    "    if return_arr:\n",
    "        return lemmatized\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def nltk_lemmatize_post_tag(word_arr, return_arr = True):\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_arr]\n",
    "    if return_arr:\n",
    "        return lemmatized\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# review_sent_word_arr - 2D; each row is a sentence in the review, and each column is a word\n",
    "def nltk_lemmatize_post_tag_sent_arr(review_sent_word_arr, return_arr = False):\n",
    "    lemmatized = []\n",
    "    # sent_words - list of words in that sentence\n",
    "    for sent_words in review_sent_word_arr:\n",
    "        lemmatized.append(\",\".join([wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in sent_words]))\n",
    "    return \";\".join(lemmatized)\n",
    "\n",
    "\n",
    "def nltk_lemmatize_post_tag_rev_words(review_words, return_arr = False):\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in review_words]\n",
    "    if return_arr:\n",
    "        return lemmatized\n",
    "    return \",\".join(lemmatized)\n",
    "\n",
    "\n",
    "# @param return_arr when False, returns list of sentences\n",
    "# where each sentence's words are lemmatized\n",
    "def sentences_words_lemmatization(sentence_words_arr, return_arr = True):\n",
    "    lemmatized = []\n",
    "    for sentence_words in sentence_words_arr:\n",
    "        sen_lemma = nltk_lemmatize_post_tag(sentence_words, return_arr)\n",
    "        lemmatized.append(sen_lemma)\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94694b",
   "metadata": {},
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a71e38e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x1a162ddcee0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.factory('en_language_detector_8')\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\"])\n",
    "# Language.factory(\"en_language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('en_language_detector_8', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d00b6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECT LANGUAGE AND CHECK WHETHER A TEXT IS ENGLISH OR NOT\n",
    "def detect_lang(text):\n",
    "    doc = nlp(text)\n",
    "    detect_language = doc._.language\n",
    "    return detect_language # {'language': 'de', 'score': 0.9999958526911192}\n",
    "def check_lang(text, lng = 'en', check_score = False, min_score = 0.8):\n",
    "    detected = detect_lang(text)\n",
    "    if check_score is False:\n",
    "        return detected['language'] == lng\n",
    "    return detected['language'] == lng and detected['score'] >= min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c35c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
