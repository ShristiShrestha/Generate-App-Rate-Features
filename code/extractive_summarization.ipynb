{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d3150b",
   "metadata": {},
   "source": [
    "### Extractive summarization using Hybrid Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- Load preprocessed reviews (after lemmatization) where each review is broken down into sentences\n",
    "- Each data input is a tokenized sentence\n",
    "- Compute TFIDF score for each term in the sentences\n",
    "- Compute average TFIDF score for each sentence and sort the sentences by this score\n",
    "- Select top num_reviews (10) sentences based on their cosine similarity\n",
    "    - Encode each word into its word embeddings using Global Vector (GloVe) word embedding. Each embedding is a vector of dimension 300. \n",
    "    - Compute average embedding for the sentence by taking an average of non-zero vectors of the words in the sentence\n",
    "    - Use threshold = (0.01, 0.99), diff_threshold = 0.01 to select dissimilar sentences having to hybrid TF-IDF score\n",
    "    - Each sentence has score above 0.01, and below 0.99. \n",
    "    - Selected sentences differ from each other by atleast 0.01 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61917d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sshre35\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run lib.ipynb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0ebdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from functools import reduce \n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9180a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained GloVe model\n",
    "glove_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78513d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_reviews = {}\n",
    "domains = [\"ride\", \"health\", \"investing\"]\n",
    "min_wc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lemmatized reviews in each domain\n",
    "for domain in domains:\n",
    "    input_csv_file = DATA_DIR + \"lemmatized/\" + domain + \"_lemmatized.csv\"\n",
    "    print(\"\\n-----loading reviews from \", input_csv_file, \"--------------\\n\")\n",
    "    df = pd.read_csv(input_csv_file)\n",
    "    df = df[df[\"word count\"] > min_wc]\n",
    "    domains_reviews[domain] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b6aadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>someone made an account on this app using my e...</td>\n",
       "      <td>someone made an account on this app using my e...</td>\n",
       "      <td>someone,made,account,app,using,email,address</td>\n",
       "      <td>someone,make,account,app,use,email,address</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone made an account on this app using my e...</td>\n",
       "      <td>i get all of their receipts, trip info, and cu...</td>\n",
       "      <td>get,receipts,trip,info,customer,service,responses</td>\n",
       "      <td>get,receipt,trip,info,customer,service,response</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>someone made an account on this app using my e...</td>\n",
       "      <td>they don't even have remotely the same name as...</td>\n",
       "      <td>even,remotely,name,emailed,several,times,ask,s...</td>\n",
       "      <td>even,remotely,name,email,several,time,ask,stop...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>someone made an account on this app using my e...</td>\n",
       "      <td>they had me describe myself to prove it's inco...</td>\n",
       "      <td>describe,prove,incorrect,still,emailing</td>\n",
       "      <td>describe,prove,incorrect,still,email</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>someone made an account on this app using my e...</td>\n",
       "      <td>they should at least have some sort of email v...</td>\n",
       "      <td>least,sort,email,verification,prove,emailing,v...</td>\n",
       "      <td>least,sort,email,verification,prove,email,vali...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  someone made an account on this app using my e...   \n",
       "1  someone made an account on this app using my e...   \n",
       "2  someone made an account on this app using my e...   \n",
       "3  someone made an account on this app using my e...   \n",
       "4  someone made an account on this app using my e...   \n",
       "\n",
       "                                                sent  \\\n",
       "0  someone made an account on this app using my e...   \n",
       "1  i get all of their receipts, trip info, and cu...   \n",
       "2  they don't even have remotely the same name as...   \n",
       "3  they had me describe myself to prove it's inco...   \n",
       "4  they should at least have some sort of email v...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0       someone,made,account,app,using,email,address   \n",
       "1  get,receipts,trip,info,customer,service,responses   \n",
       "2  even,remotely,name,emailed,several,times,ask,s...   \n",
       "3            describe,prove,incorrect,still,emailing   \n",
       "4  least,sort,email,verification,prove,emailing,v...   \n",
       "\n",
       "                                          lemmatized  word count  \n",
       "0         someone,make,account,app,use,email,address           7  \n",
       "1    get,receipt,trip,info,customer,service,response           7  \n",
       "2  even,remotely,name,email,several,time,ask,stop...           9  \n",
       "3               describe,prove,incorrect,still,email           5  \n",
       "4  least,sort,email,verification,prove,email,vali...           9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_reviews[\"ride\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162cead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i like investing on here. yeah, savings are ma...</td>\n",
       "      <td>yeah, savings are mainly through round-ups, bu...</td>\n",
       "      <td>yeah,savings,mainly,roundups,saved,lot</td>\n",
       "      <td>yeah,saving,mainly,roundup,save,lot</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i like investing on here. yeah, savings are ma...</td>\n",
       "      <td>the spend account, however, neither makes sens...</td>\n",
       "      <td>spend,account,however,neither,makes,sense,conn...</td>\n",
       "      <td>spend,account,however,neither,make,sense,conne...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i like investing on here. yeah, savings are ma...</td>\n",
       "      <td>you would think you could just move money betw...</td>\n",
       "      <td>would,think,could,move,money,accounts,really,o...</td>\n",
       "      <td>would,think,could,move,money,account,really,on...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i like investing on here. yeah, savings are ma...</td>\n",
       "      <td>most people signed up using another bank.</td>\n",
       "      <td>people,signed,using,another,bank</td>\n",
       "      <td>people,sign,use,another,bank</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i like investing on here. yeah, savings are ma...</td>\n",
       "      <td>you can't even move invest funds to spend.</td>\n",
       "      <td>even,move,invest,funds,spend</td>\n",
       "      <td>even,move,invest,fund,spend</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "1  i like investing on here. yeah, savings are ma...   \n",
       "2  i like investing on here. yeah, savings are ma...   \n",
       "3  i like investing on here. yeah, savings are ma...   \n",
       "5  i like investing on here. yeah, savings are ma...   \n",
       "7  i like investing on here. yeah, savings are ma...   \n",
       "\n",
       "                                                sent  \\\n",
       "1  yeah, savings are mainly through round-ups, bu...   \n",
       "2  the spend account, however, neither makes sens...   \n",
       "3  you would think you could just move money betw...   \n",
       "5          most people signed up using another bank.   \n",
       "7         you can't even move invest funds to spend.   \n",
       "\n",
       "                                           tokenized  \\\n",
       "1             yeah,savings,mainly,roundups,saved,lot   \n",
       "2  spend,account,however,neither,makes,sense,conn...   \n",
       "3  would,think,could,move,money,accounts,really,o...   \n",
       "5                   people,signed,using,another,bank   \n",
       "7                       even,move,invest,funds,spend   \n",
       "\n",
       "                                          lemmatized  word count  \n",
       "1                yeah,saving,mainly,roundup,save,lot           6  \n",
       "2  spend,account,however,neither,make,sense,conne...          10  \n",
       "3  would,think,could,move,money,account,really,on...          11  \n",
       "5                       people,sign,use,another,bank           5  \n",
       "7                        even,move,invest,fund,spend           5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_reviews[\"investing\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4297081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i've tried many sleep apps the last few years,...</td>\n",
       "      <td>i've tried many sleep apps the last few years,...</td>\n",
       "      <td>tried,many,sleep,apps,last,years,helped,extent...</td>\n",
       "      <td>try,many,sleep,apps,last,year,help,extent,none...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i've tried many sleep apps the last few years,...</td>\n",
       "      <td>there are dozens - maybe hundreds - to choose ...</td>\n",
       "      <td>dozens,maybe,hundreds,choose,tell,within,minut...</td>\n",
       "      <td>dozen,maybe,hundred,choose,tell,within,minute,...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i've tried many sleep apps the last few years,...</td>\n",
       "      <td>in the trial period, i was already figuring ou...</td>\n",
       "      <td>trial,period,already,figuring,individuals,want...</td>\n",
       "      <td>trial,period,already,figure,individual,want,fo...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i've tried many sleep apps the last few years,...</td>\n",
       "      <td>in the past, lying with my eyes closed trying ...</td>\n",
       "      <td>past,lying,eyes,closed,trying,sleep,would,wind...</td>\n",
       "      <td>past,lie,eye,close,try,sleep,would,wind,make,f...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've tried many sleep apps the last few years,...</td>\n",
       "      <td>listening to the hypnosis audio that i like ta...</td>\n",
       "      <td>listening,hypnosis,audio,like,takes,place,open...</td>\n",
       "      <td>listen,hypnosis,audio,like,take,place,openness...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  i've tried many sleep apps the last few years,...   \n",
       "1  i've tried many sleep apps the last few years,...   \n",
       "2  i've tried many sleep apps the last few years,...   \n",
       "3  i've tried many sleep apps the last few years,...   \n",
       "4  i've tried many sleep apps the last few years,...   \n",
       "\n",
       "                                                sent  \\\n",
       "0  i've tried many sleep apps the last few years,...   \n",
       "1  there are dozens - maybe hundreds - to choose ...   \n",
       "2  in the trial period, i was already figuring ou...   \n",
       "3  in the past, lying with my eyes closed trying ...   \n",
       "4  listening to the hypnosis audio that i like ta...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  tried,many,sleep,apps,last,years,helped,extent...   \n",
       "1  dozens,maybe,hundreds,choose,tell,within,minut...   \n",
       "2  trial,period,already,figuring,individuals,want...   \n",
       "3  past,lying,eyes,closed,trying,sleep,would,wind...   \n",
       "4  listening,hypnosis,audio,like,takes,place,open...   \n",
       "\n",
       "                                          lemmatized  word count  \n",
       "0  try,many,sleep,apps,last,year,help,extent,none...          14  \n",
       "1  dozen,maybe,hundred,choose,tell,within,minute,...          10  \n",
       "2  trial,period,already,figure,individual,want,fo...          12  \n",
       "3  past,lie,eye,close,try,sleep,would,wind,make,f...          11  \n",
       "4  listen,hypnosis,audio,like,take,place,openness...          10  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_reviews[\"health\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a66c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix (corpus_size, num_features) : term frequency of unique words in each document\n",
    "# df also known as document frequency (num_features) : counts number of doc that contains the given word\n",
    "\n",
    "class HybridTfidfTransformer:\n",
    "    def __init__(self, len_unique_words, corpus_size):\n",
    "        self.len_unique_words = len_unique_words\n",
    "        self.corpus_size = corpus_size # size of corpus\n",
    "\n",
    "    def transform(self, matrix, df):\n",
    "        tf = matrix / self.len_unique_words # normalize term frequency by the number of unique words in the corpus\n",
    "        idf = np.log(self.corpus_size / df)\n",
    "        tf_idf = tf * idf\n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a0177c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def create_glove_embeddings(reviews, review_sent = False):\n",
    "    \n",
    "    review_embeddings = []\n",
    "    for review_str in reviews:\n",
    "        review_words = review_str\n",
    "        top_words = []\n",
    "        valid_words = 0\n",
    "        if review_sent:\n",
    "            review_words = nltk.word_tokenize(review_str)\n",
    "        # get embeddings for each word\n",
    "        review_words_embedding = []\n",
    "        for word in review_words:\n",
    "            if word in glove_model.key_to_index:\n",
    "                review_words_embedding.append(glove_model[word])\n",
    "                valid_words += 1\n",
    "            else:\n",
    "                review_words_embedding.append(np.zeros(300))\n",
    "        review_embedding = np.mean(review_words_embedding, axis=0)\n",
    "        #if review_embedding is nan\n",
    "        if len(review_embedding.shape) == 0: \n",
    "            review_embedding = np.zeros(300)\n",
    "        else:\n",
    "            # centroid embedding of this review with associated words\n",
    "            sim = cosine_similarity([review_embedding], review_words_embedding)[0]\n",
    "            # get top 80 words closer to sentence embedding\n",
    "            top_words_indices =sim.argsort()[::-1][:int(0.8*valid_words)]\n",
    "            top_words = set([review_words[i] for i in top_words_indices])\n",
    "        \n",
    "        review_embeddings.append((review_embedding, \",\".join(top_words)))\n",
    "\n",
    "    # Normalize the review embeddings\n",
    "    review_embeddings_arr = np.array([embedding for embedding, top_words in review_embeddings])\n",
    "    review_embeddings_arr /= np.linalg.norm(review_embeddings_arr, axis=1).reshape(-1, 1)\n",
    "    review_embeddings_top_words = [top_words for embedding, top_words in review_embeddings]\n",
    "    return (review_embeddings_arr, review_embeddings_top_words)\n",
    "\n",
    "\n",
    "    \n",
    "def hybrid_tfidf_sentences_summary(raw_reviews, raw_sent, lemma_reviews, \n",
    "                                   output_file, num_reviews = 20, \n",
    "                                   threshold = (0.01, 0.99), diff_threshold = 0.01):\n",
    "\n",
    "    review_words_str = [\" \".join(words) for words in lemma_reviews]\n",
    "\n",
    "    # create tfidf object and fit it with reviews\n",
    "    vectorizer = TfidfVectorizer(norm=None) # # do not apply L2 normalization to TF-IDF scores\n",
    "    tfidf_matrix = vectorizer.fit_transform(review_words_str)\n",
    "    \n",
    "    # create a custom transformer for the hybrid TF-IDF formula\n",
    "    corpus_size = len(lemma_reviews)\n",
    "    len_unique_words = len(vectorizer.idf_)\n",
    "    hybrid_transformer = HybridTfidfTransformer(len_unique_words, corpus_size)\n",
    "\n",
    "    # calculate the document frequencies (df) for each feature\n",
    "    df = np.array((tfidf_matrix != 0).sum(axis=0)).flatten()\n",
    "    \n",
    "    # apply the custom transformer to the TF-IDF vectors to obtain the hybrid TF-IDF scores\n",
    "    sentence_scores = hybrid_transformer.transform(tfidf_matrix, df)\n",
    "    \n",
    "    review_embeddings, review_embeddings_top_words = create_glove_embeddings(lemma_reviews)\n",
    "    sentence_embeddings = np.array(review_embeddings)\n",
    "\n",
    "    \n",
    "    # Generate the summary by selecting the top sentences\n",
    "    selected_indices = np.argsort(sentence_scores)[::-1]\n",
    "    \n",
    "    index = 0\n",
    "    next_index = index + 1\n",
    "    review_index = selected_indices[0]\n",
    "    count = 1\n",
    "    dissimilar_reviews = [(review_index, sentence_scores[review_index], \n",
    "                           raw_reviews[review_index], \n",
    "                           raw_sent[review_index],\n",
    "                           \",\".join(lemma_reviews[review_index]))]\n",
    "    last_sim = 1\n",
    "    \n",
    "    while count < num_reviews:\n",
    "        if next_index >= num_reviews:\n",
    "            break\n",
    "        next_review_index = selected_indices[next_index]\n",
    "        review_matrix = [sentence_embeddings[review_index], sentence_embeddings[next_review_index]]\n",
    "        cosine_value = cosine_similarity(review_matrix)[0][1]\n",
    "        diff_last_sim = abs(last_sim - cosine_value)\n",
    "        if diff_last_sim > diff_threshold and cosine_value > threshold[0] and cosine_value < threshold[1]:\n",
    "            dissimilar_reviews.append((next_review_index, sentence_scores[next_review_index], \n",
    "                                       raw_reviews[next_review_index], \n",
    "                                       raw_sent[next_review_index],\n",
    "                                       \",\".join(lemma_reviews[next_review_index])))\n",
    "            count += 1\n",
    "            last_sim = cosine_value\n",
    "        next_index += 1\n",
    "    \n",
    "    print(\"------------saving output to file\", output_file, \"------------\")\n",
    "    df = pd.DataFrame(dissimilar_reviews, columns = [\"doc_id\", \"score\", \"raw\", \"sent\",  \"lemmatized\"])\n",
    "    df.to_csv(output_file, header=True, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3198c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_summary = {}\n",
    "\n",
    "for domain in domains:\n",
    "    df = domains_reviews[domain]\n",
    "    raw_reviews = df[\"review\"].tolist()\n",
    "    sent_reviews = df[\"sent\"].tolist()\n",
    "    lemma_reviews = df[\"lemmatized\"].tolist()\n",
    "    lemmatized_reviews = [item.split(\",\") for item in lemma_reviews]\n",
    "    output_file = DATA_DIR + domain + \"_summary.csv\"\n",
    "    summary_df = hybrid_tfidf_sentences_summary(raw_reviews, sent_reviews, lemmatized_reviews, output_file, 15)\n",
    "    domains_summary[domain] = summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "153627cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "      <th>raw</th>\n",
       "      <th>sent</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27783</td>\n",
       "      <td>1.205823</td>\n",
       "      <td>i am afairlynew lyftriderand iheardheard that ...</td>\n",
       "      <td>i am afairlynew lyftriderand iheardheard that ...</td>\n",
       "      <td>competition,lyft,find,true,ber,seem,much,well,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26214</td>\n",
       "      <td>1.180414</td>\n",
       "      <td>usually cheaper than uber not cheaper than a c...</td>\n",
       "      <td>usually cheaper than uber not cheaper than a c...</td>\n",
       "      <td>usually,cheaper,uber,cheaper,cab,reliable,usua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62785</td>\n",
       "      <td>0.802241</td>\n",
       "      <td>i hate via they take way to long they take you...</td>\n",
       "      <td>i hate via they take way to long they take you...</td>\n",
       "      <td>hate,via,take,way,long,take,money,paid,via,pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35641</td>\n",
       "      <td>0.668979</td>\n",
       "      <td>i ordered a lift to  night an d they told me i...</td>\n",
       "      <td>i ordered a lift to  night an d they told me i...</td>\n",
       "      <td>order,lift,night,told,would,want,get,text,tell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42767</td>\n",
       "      <td>0.646816</td>\n",
       "      <td>i was hesitant to book an uber bc of all the h...</td>\n",
       "      <td>i was hesitant to book an uber bc of all the h...</td>\n",
       "      <td>hesitant,book,uber,horror,story,heard,ubers,st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id     score                                                raw  \\\n",
       "0   27783  1.205823  i am afairlynew lyftriderand iheardheard that ...   \n",
       "1   26214  1.180414  usually cheaper than uber not cheaper than a c...   \n",
       "2   62785  0.802241  i hate via they take way to long they take you...   \n",
       "3   35641  0.668979  i ordered a lift to  night an d they told me i...   \n",
       "4   42767  0.646816  i was hesitant to book an uber bc of all the h...   \n",
       "\n",
       "                                                sent  \\\n",
       "0  i am afairlynew lyftriderand iheardheard that ...   \n",
       "1  usually cheaper than uber not cheaper than a c...   \n",
       "2  i hate via they take way to long they take you...   \n",
       "3  i ordered a lift to  night an d they told me i...   \n",
       "4  i was hesitant to book an uber bc of all the h...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  competition,lyft,find,true,ber,seem,much,well,...  \n",
       "1  usually,cheaper,uber,cheaper,cab,reliable,usua...  \n",
       "2  hate,via,take,way,long,take,money,paid,via,pas...  \n",
       "3  order,lift,night,told,would,want,get,text,tell...  \n",
       "4  hesitant,book,uber,horror,story,heard,ubers,st...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_summary[\"ride\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d65b5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "      <th>raw</th>\n",
       "      <th>sent</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25004</td>\n",
       "      <td>0.782249</td>\n",
       "      <td>had to go back &amp; forth with customer service t...</td>\n",
       "      <td>had to go back &amp; forth with customer service t...</td>\n",
       "      <td>back,forth,customer,service,get,price,two,frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63148</td>\n",
       "      <td>0.705859</td>\n",
       "      <td>just downloaded rootd today &amp; i think itz such...</td>\n",
       "      <td>just downloaded rootd today &amp; i think itz such...</td>\n",
       "      <td>download,rootd,today,think,cute,lil,app,help,m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40152</td>\n",
       "      <td>0.580661</td>\n",
       "      <td>this app is gawd awful!! i accidentally downlo...</td>\n",
       "      <td>i accidentally downloaded it(via my niece) and...</td>\n",
       "      <td>accidentally,download,via,niece,get,update,say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26196</td>\n",
       "      <td>0.440951</td>\n",
       "      <td>this app is for anxiety and to help people who...</td>\n",
       "      <td>this app is for anxiety and to help people who...</td>\n",
       "      <td>app,anxiety,help,people,trouble,fall,asleep,be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4375</td>\n",
       "      <td>0.400531</td>\n",
       "      <td>i've been using betterhelp for maybe six month...</td>\n",
       "      <td>i've been using betterhelp for maybe six month...</td>\n",
       "      <td>use,betterhelp,maybe,six,month,student,discoun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id     score                                                raw  \\\n",
       "0   25004  0.782249  had to go back & forth with customer service t...   \n",
       "1   63148  0.705859  just downloaded rootd today & i think itz such...   \n",
       "2   40152  0.580661  this app is gawd awful!! i accidentally downlo...   \n",
       "3   26196  0.440951  this app is for anxiety and to help people who...   \n",
       "4    4375  0.400531  i've been using betterhelp for maybe six month...   \n",
       "\n",
       "                                                sent  \\\n",
       "0  had to go back & forth with customer service t...   \n",
       "1  just downloaded rootd today & i think itz such...   \n",
       "2  i accidentally downloaded it(via my niece) and...   \n",
       "3  this app is for anxiety and to help people who...   \n",
       "4  i've been using betterhelp for maybe six month...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  back,forth,customer,service,get,price,two,frie...  \n",
       "1  download,rootd,today,think,cute,lil,app,help,m...  \n",
       "2  accidentally,download,via,niece,get,update,say...  \n",
       "3  app,anxiety,help,people,trouble,fall,asleep,be...  \n",
       "4  use,betterhelp,maybe,six,month,student,discoun...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_summary[\"health\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4992a356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "      <th>raw</th>\n",
       "      <th>sent</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37683</td>\n",
       "      <td>0.806133</td>\n",
       "      <td>m1 vs webull hands down m1 is better then webu...</td>\n",
       "      <td>webull has a stock lending program allows you ...</td>\n",
       "      <td>webull,stock,lending,program,allows,loan,share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33941</td>\n",
       "      <td>0.743765</td>\n",
       "      <td>i am a disabled american man that was excited ...</td>\n",
       "      <td>i am a disabled american man that was excited ...</td>\n",
       "      <td>disabled,american,man,excite,opening,account,p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26125</td>\n",
       "      <td>0.624518</td>\n",
       "      <td>i gave one star because had to do something to...</td>\n",
       "      <td>i gave one star because had to do something to...</td>\n",
       "      <td>give,one,star,something,proceed,download,app,p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>223916</td>\n",
       "      <td>0.538981</td>\n",
       "      <td>i enjoy trading on webull but mobile app and d...</td>\n",
       "      <td>i enjoy trading on webull but mobile app and d...</td>\n",
       "      <td>enjoy,trading,webull,mobile,app,desktop,versio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26484</td>\n",
       "      <td>0.513329</td>\n",
       "      <td>nothing works on apple since march 2020 - not ...</td>\n",
       "      <td>an issue with broken watch lists on apple ipad...</td>\n",
       "      <td>issue,broken,watch,list,apple,ipad,fidelity,ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id     score                                                raw  \\\n",
       "0   37683  0.806133  m1 vs webull hands down m1 is better then webu...   \n",
       "1   33941  0.743765  i am a disabled american man that was excited ...   \n",
       "2   26125  0.624518  i gave one star because had to do something to...   \n",
       "3  223916  0.538981  i enjoy trading on webull but mobile app and d...   \n",
       "4   26484  0.513329  nothing works on apple since march 2020 - not ...   \n",
       "\n",
       "                                                sent  \\\n",
       "0  webull has a stock lending program allows you ...   \n",
       "1  i am a disabled american man that was excited ...   \n",
       "2  i gave one star because had to do something to...   \n",
       "3  i enjoy trading on webull but mobile app and d...   \n",
       "4  an issue with broken watch lists on apple ipad...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  webull,stock,lending,program,allows,loan,share...  \n",
       "1  disabled,american,man,excite,opening,account,p...  \n",
       "2  give,one,star,something,proceed,download,app,p...  \n",
       "3  enjoy,trading,webull,mobile,app,desktop,versio...  \n",
       "4  issue,broken,watch,list,apple,ipad,fidelity,ap...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_summary[\"investing\"].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
